{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text_Sentiment_Classification.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1eIAuTrixV1hUu5cFbfZ2BCLNhWAtIPB5","authorship_tag":"ABX9TyNfRaCZ03YGTl037Sv3J1ab"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0ovtdgJvLVUY"},"source":["导包"]},{"cell_type":"code","metadata":{"id":"CQkwt6TxrL6E","executionInfo":{"status":"ok","timestamp":1624236211285,"user_tz":-480,"elapsed":4066,"user":{"displayName":"Tracy Xu","photoUrl":"","userId":"07997212518309303637"}}},"source":["import collections\n","import os\n","import random\n","import time\n","from tqdm import tqdm\n","import torch\n","from torch import nn\n","import torchtext.vocab as Vocab\n","import torch.utils.data as Data\n","import torch.nn.functional as F\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D_GVaWBQLYgt"},"source":["解压数据集"]},{"cell_type":"code","metadata":{"id":"m0hlcaerA37a"},"source":["!cp -r /content/drive/My\\ Drive/Text\\ Sentiment\\ Classification/aclImdb_v1.zip ./  #将google云盘中的数据集压缩文件拷贝到当前运行环境\n","!unzip aclImdb_v1.zip  #将数据集压缩文件解压"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v134e_uTLd_5"},"source":["导入数据集"]},{"cell_type":"code","metadata":{"id":"IrWLapkrB89a"},"source":["def read_imdb(data_root, folder='train'):\n","    data = []\n","    for label in ['pos', 'neg']:\n","        folder_name = os.path.join(folder, data_root, label)\n","        for file in tqdm(os.listdir(folder_name)):\n","            with open(os.path.join(folder_name, file), 'rb') as f:\n","                review = f.read().decode('utf-8').replace('\\n', '').lower()\n","                data.append([review, 1 if label == 'pos' else 0])\n","    random.shuffle(data)\n","    return data\n","\n","DATA_ROOT = 'aclImdb_v1/'\n","data_root = os.path.join(DATA_ROOT, \"aclImdb\")\n","train_data, test_data = read_imdb('train', data_root), read_imdb('test', data_root)\n","\n","# 打印训练数据中的前五个sample\n","for sample in train_data[:5]:\n","    print(sample[1], '\\t', sample[0][:50])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2kFvD1UnLyJ8"},"source":["数据预处理"]},{"cell_type":"code","metadata":{"id":"COP4fcwmEX_o"},"source":["def get_tokenized_imdb(data):\n","    '''\n","    @params:\n","        data: 数据的列表，列表中的每个元素为 [文本字符串，0/1标签] 二元组\n","    @return: 切分词后的文本的列表，列表中的每个元素为切分后的词序列\n","    '''\n","    def tokenizer(text):\n","        return [tok.lower() for tok in text.split(' ')]\n","    \n","    return [tokenizer(review) for review, _ in data]\n","\n","def get_vocab_imdb(data):\n","    '''\n","    @params:\n","        data: 同上\n","    @return: 数据集上的词典，Vocab 的实例（freqs, stoi, itos）\n","    '''\n","    tokenized_data = get_tokenized_imdb(data)\n","    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n","    return Vocab.Vocab(counter)\n","\n","vocab = get_vocab_imdb(train_data)\n","print('# words in vocab:', len(vocab))\n","# words in vocab: 46152"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WkkjLQaLMBgp"},"source":["词典和词语的索引创建好后，就可以将数据集的文本从字符串的形式转换为单词下标序列的形式，以待之后的使用。"]},{"cell_type":"code","metadata":{"id":"yuZiY4EIGg0w","executionInfo":{"status":"ok","timestamp":1624237548005,"user_tz":-480,"elapsed":457,"user":{"displayName":"Tracy Xu","photoUrl":"","userId":"07997212518309303637"}}},"source":["def preprocess_imdb(data, vocab):\n","    '''\n","    @params:\n","        data: 同上，原始的读入数据\n","        vocab: 训练集上生成的词典\n","    @return:\n","        features: 单词下标序列，形状为 (n, max_l) 的整数张量\n","        labels: 情感标签，形状为 (n,) 的0/1整数张量\n","    '''\n","    max_l = 500  # 将每条评论通过截断或者补0，使得长度变成500\n","\n","    def pad(x):\n","        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n","\n","    tokenized_data = get_tokenized_imdb(data)\n","    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n","    labels = torch.tensor([score for _, score in data])\n","    return features, labels"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"29pdvjLQMFZK"},"source":["创建数据迭代器"]},{"cell_type":"code","metadata":{"id":"0J3N5GikKsxI"},"source":["train_set = Data.TensorDataset(*preprocess_imdb(train_data, vocab))\n","test_set = Data.TensorDataset(*preprocess_imdb(test_data, vocab))\n","\n","# 上面的代码等价于下面的注释代码\n","# train_features, train_labels = preprocess_imdb(train_data, vocab)\n","# test_features, test_labels = preprocess_imdb(test_data, vocab)\n","# train_set = Data.TensorDataset(train_features, train_labels)\n","# test_set = Data.TensorDataset(test_features, test_labels)\n","\n","# len(train_set) = features.shape[0] or labels.shape[0]\n","# train_set[index] = (features[index], labels[index])\n","\n","batch_size = 64\n","train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n","test_iter = Data.DataLoader(test_set, batch_size)\n","\n","for X, y in train_iter:\n","    print('X', X.shape, 'y', y.shape)\n","    break\n","print('#batches:', len(train_iter))\n","# X torch.Size([64, 500]) y torch.Size([64])\n","# batches: 391"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6L950gO6MO3s"},"source":["利用 torch.nn.RNN 或 torch.nn.LSTM 模组，可以很方便地实现双向循环神经网络，下面是以 LSTM 为例的代码。"]},{"cell_type":"code","metadata":{"id":"2MNb-7YqK8bY"},"source":["class BiRNN(nn.Module):\n","    def __init__(self, vocab, embed_size, num_hiddens, num_layers):\n","        '''\n","        @params:\n","            vocab: 在数据集上创建的词典，用于获取词典大小\n","            embed_size: 嵌入维度大小\n","            num_hiddens: 隐藏状态维度大小\n","            num_layers: 隐藏层个数\n","        '''\n","        super(BiRNN, self).__init__()\n","        self.embedding = nn.Embedding(len(vocab), embed_size)\n","        \n","        # encoder-decoder framework\n","        # bidirectional设为True即得到双向循环神经网络\n","        self.encoder = nn.LSTM(input_size=embed_size, \n","                                hidden_size=num_hiddens, \n","                                num_layers=num_layers,\n","                                bidirectional=True)\n","        self.decoder = nn.Linear(4*num_hiddens, 2) # 初始时间步和最终时间步的隐藏状态作为全连接层输入\n","        \n","    def forward(self, inputs):\n","        '''\n","        @params:\n","            inputs: 词语下标序列，形状为 (batch_size, seq_len) 的整数张量\n","        @return:\n","            outs: 对文本情感的预测，形状为 (batch_size, 2) 的张量\n","        '''\n","        # 因为LSTM需要将序列长度(seq_len)作为第一维，所以需要将输入转置\n","        embeddings = self.embedding(inputs.permute(1, 0)) # (seq_len, batch_size, d)\n","        # rnn.LSTM 返回输出、隐藏状态和记忆单元，格式如 outputs, (h, c)\n","        outputs, _ = self.encoder(embeddings) # (seq_len, batch_size, 2*h)\n","        encoding = torch.cat((outputs[0], outputs[-1]), -1) # (batch_size, 4*h)\n","        outs = self.decoder(encoding) # (batch_size, 2)\n","        return outs\n","\n","embed_size, num_hiddens, num_layers = 100, 100, 2\n","net = BiRNN(vocab, embed_size, num_hiddens, num_layers)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ebawr5OMMTVW"},"source":["加载预训练的词向量"]},{"cell_type":"code","metadata":{"id":"ZvAIw_ZJK-P3"},"source":["cache_dir = \"/home/kesci/input/GloVe6B5429\"\n","glove_vocab = Vocab.GloVe(name='6B', dim=100, cache=cache_dir)\n","\n","def load_pretrained_embedding(words, pretrained_vocab):\n","    '''\n","    @params:\n","        words: 需要加载词向量的词语列表，以 itos (index to string) 的词典形式给出\n","        pretrained_vocab: 预训练词向量\n","    @return:\n","        embed: 加载到的词向量\n","    '''\n","    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # 初始化为0\n","    oov_count = 0 # out of vocabulary\n","    for i, word in enumerate(words):\n","        try:\n","            idx = pretrained_vocab.stoi[word]\n","            embed[i, :] = pretrained_vocab.vectors[idx]\n","        except KeyError:\n","            oov_count += 1\n","    if oov_count > 0:\n","        print(\"There are %d oov words.\" % oov_count)\n","    return embed\n","\n","net.embedding.weight.data.copy_(load_pretrained_embedding(vocab.itos, glove_vocab))\n","net.embedding.weight.requires_grad = False # 直接加载预训练好的, 所以不需要更新它"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gXlsP8aCMXQg"},"source":["训练模型"]},{"cell_type":"code","metadata":{"id":"3q5h7pdzLCV9"},"source":["def evaluate_accuracy(data_iter, net, device=None):\n","    if device is None and isinstance(net, torch.nn.Module):\n","        device = list(net.parameters())[0].device \n","    acc_sum, n = 0.0, 0\n","    with torch.no_grad():\n","        for X, y in data_iter:\n","            if isinstance(net, torch.nn.Module):\n","                net.eval()\n","                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n","                net.train()\n","            else:\n","                if('is_training' in net.__code__.co_varnames):\n","                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n","                else:\n","                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n","            n += y.shape[0]\n","    return acc_sum / n\n","\n","def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n","    net = net.to(device)\n","    print(\"training on \", device)\n","    batch_count = 0\n","    for epoch in range(num_epochs):\n","        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n","        for X, y in train_iter:\n","            X = X.to(device)\n","            y = y.to(device)\n","            y_hat = net(X)\n","            l = loss(y_hat, y) \n","            optimizer.zero_grad()\n","            l.backward()\n","            optimizer.step()\n","            train_l_sum += l.cpu().item()\n","            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n","            n += y.shape[0]\n","            batch_count += 1\n","        test_acc = evaluate_accuracy(test_iter, net)\n","        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n","              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7SLuoI1BMb-M"},"source":["由于嵌入层的参数是不需要在训练过程中被更新的，所以我们利用 filter 函数和 lambda 表达式来过滤掉模型中不需要更新参数的部分。"]},{"cell_type":"code","metadata":{"id":"cbobEgU4LGHH"},"source":["lr, num_epochs = 0.01, 5\n","optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n","loss = nn.CrossEntropyLoss()\n","\n","train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w2FNlebrMhN_"},"source":["评价模型"]},{"cell_type":"code","metadata":{"id":"BkB-XnEFLJEt"},"source":["def predict_sentiment(net, vocab, sentence):\n","    '''\n","    @params：\n","        net: 训练好的模型\n","        vocab: 在该数据集上创建的词典，用于将给定的单词序转换为单词下标的序列，从而输入模型\n","        sentence: 需要分析情感的文本，以单词序列的形式给出\n","    @return: 预测的结果，positive 为正面情绪文本，negative 为负面情绪文本\n","    '''\n","    device = list(net.parameters())[0].device # 读取模型所在的环境\n","    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\n","    label = torch.argmax(net(sentence.view((1, -1))), dim=1)\n","    return 'positive' if label.item() == 1 else 'negative'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IITXjAurLQVH"},"source":["predict_sentiment(net, vocab, ['this', 'movie', 'is', 'so', 'great'])\n","# 'positive'\n","\n","predict_sentiment(net, vocab, ['this', 'movie', 'is', 'so', 'bad'])\n","# 'negative'"],"execution_count":null,"outputs":[]}]}